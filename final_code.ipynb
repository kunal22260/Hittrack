{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload dataset \n",
    "# drop columns as in code.ipynb \n",
    "# encoding like code.ipynb\n",
    "# do train test split first \n",
    "# take the train part and do all preprocessing and save models \n",
    "# -> use min max scaler \n",
    "# -> add K means label find using silhouette "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_preprocessed_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_duration_ms</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>audio_mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>audio_valence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.736757</td>\n",
       "      <td>-0.876374</td>\n",
       "      <td>-0.876400</td>\n",
       "      <td>0.172845</td>\n",
       "      <td>-0.351913</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>-0.838658</td>\n",
       "      <td>0.875829</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>-0.696514</td>\n",
       "      <td>1.601565</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-0.220610</td>\n",
       "      <td>Popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.021350</td>\n",
       "      <td>-0.859818</td>\n",
       "      <td>-0.582880</td>\n",
       "      <td>0.971555</td>\n",
       "      <td>-0.352046</td>\n",
       "      <td>-0.633337</td>\n",
       "      <td>-0.497639</td>\n",
       "      <td>0.271815</td>\n",
       "      <td>-1.299684</td>\n",
       "      <td>-0.501066</td>\n",
       "      <td>-0.550858</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-0.645751</td>\n",
       "      <td>Mildly Popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.225786</td>\n",
       "      <td>-0.867195</td>\n",
       "      <td>0.661389</td>\n",
       "      <td>-0.850065</td>\n",
       "      <td>1.665241</td>\n",
       "      <td>-1.463328</td>\n",
       "      <td>0.523333</td>\n",
       "      <td>-0.099423</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>-0.219391</td>\n",
       "      <td>0.097788</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-0.833793</td>\n",
       "      <td>Popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.021350</td>\n",
       "      <td>-0.804053</td>\n",
       "      <td>-1.163539</td>\n",
       "      <td>1.518042</td>\n",
       "      <td>-0.336025</td>\n",
       "      <td>-1.463328</td>\n",
       "      <td>-0.539312</td>\n",
       "      <td>0.655594</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>0.046954</td>\n",
       "      <td>0.047742</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-1.348867</td>\n",
       "      <td>Popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.093752</td>\n",
       "      <td>-0.892189</td>\n",
       "      <td>-1.189062</td>\n",
       "      <td>0.565194</td>\n",
       "      <td>-0.352046</td>\n",
       "      <td>1.303308</td>\n",
       "      <td>-0.462913</td>\n",
       "      <td>0.622415</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>-0.678310</td>\n",
       "      <td>1.773991</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>0.188178</td>\n",
       "      <td>Potential Flop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   song_duration_ms  acousticness  danceability    energy  instrumentalness  \\\n",
       "0          0.736757     -0.876374     -0.876400  0.172845         -0.351913   \n",
       "1         -0.021350     -0.859818     -0.582880  0.971555         -0.352046   \n",
       "2          0.225786     -0.867195      0.661389 -0.850065          1.665241   \n",
       "3         -0.021350     -0.804053     -1.163539  1.518042         -0.336025   \n",
       "4          0.093752     -0.892189     -1.189062  0.565194         -0.352046   \n",
       "\n",
       "        key  liveness  loudness  audio_mode  speechiness     tempo  \\\n",
       "0  0.749981 -0.838658  0.875829    0.769418    -0.696514  1.601565   \n",
       "1 -0.633337 -0.497639  0.271815   -1.299684    -0.501066 -0.550858   \n",
       "2 -1.463328  0.523333 -0.099423    0.769418    -0.219391  0.097788   \n",
       "3 -1.463328 -0.539312  0.655594    0.769418     0.046954  0.047742   \n",
       "4  1.303308 -0.462913  0.622415    0.769418    -0.678310  1.773991   \n",
       "\n",
       "   time_signature  audio_valence           label  \n",
       "0        0.136944      -0.220610         Popular  \n",
       "1        0.136944      -0.645751  Mildly Popular  \n",
       "2        0.136944      -0.833793         Popular  \n",
       "3        0.136944      -1.348867         Popular  \n",
       "4        0.136944       0.188178  Potential Flop  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_duration_ms</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>audio_mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>audio_valence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.736757</td>\n",
       "      <td>-0.876374</td>\n",
       "      <td>-0.876400</td>\n",
       "      <td>0.172845</td>\n",
       "      <td>-0.351913</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>-0.838658</td>\n",
       "      <td>0.875829</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>-0.696514</td>\n",
       "      <td>1.601565</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-0.220610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.021350</td>\n",
       "      <td>-0.859818</td>\n",
       "      <td>-0.582880</td>\n",
       "      <td>0.971555</td>\n",
       "      <td>-0.352046</td>\n",
       "      <td>-0.633337</td>\n",
       "      <td>-0.497639</td>\n",
       "      <td>0.271815</td>\n",
       "      <td>-1.299684</td>\n",
       "      <td>-0.501066</td>\n",
       "      <td>-0.550858</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-0.645751</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.225786</td>\n",
       "      <td>-0.867195</td>\n",
       "      <td>0.661389</td>\n",
       "      <td>-0.850065</td>\n",
       "      <td>1.665241</td>\n",
       "      <td>-1.463328</td>\n",
       "      <td>0.523333</td>\n",
       "      <td>-0.099423</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>-0.219391</td>\n",
       "      <td>0.097788</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-0.833793</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.021350</td>\n",
       "      <td>-0.804053</td>\n",
       "      <td>-1.163539</td>\n",
       "      <td>1.518042</td>\n",
       "      <td>-0.336025</td>\n",
       "      <td>-1.463328</td>\n",
       "      <td>-0.539312</td>\n",
       "      <td>0.655594</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>0.046954</td>\n",
       "      <td>0.047742</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-1.348867</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.093752</td>\n",
       "      <td>-0.892189</td>\n",
       "      <td>-1.189062</td>\n",
       "      <td>0.565194</td>\n",
       "      <td>-0.352046</td>\n",
       "      <td>1.303308</td>\n",
       "      <td>-0.462913</td>\n",
       "      <td>0.622415</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>-0.678310</td>\n",
       "      <td>1.773991</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>0.188178</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   song_duration_ms  acousticness  danceability    energy  instrumentalness  \\\n",
       "0          0.736757     -0.876374     -0.876400  0.172845         -0.351913   \n",
       "1         -0.021350     -0.859818     -0.582880  0.971555         -0.352046   \n",
       "2          0.225786     -0.867195      0.661389 -0.850065          1.665241   \n",
       "3         -0.021350     -0.804053     -1.163539  1.518042         -0.336025   \n",
       "4          0.093752     -0.892189     -1.189062  0.565194         -0.352046   \n",
       "\n",
       "        key  liveness  loudness  audio_mode  speechiness     tempo  \\\n",
       "0  0.749981 -0.838658  0.875829    0.769418    -0.696514  1.601565   \n",
       "1 -0.633337 -0.497639  0.271815   -1.299684    -0.501066 -0.550858   \n",
       "2 -1.463328  0.523333 -0.099423    0.769418    -0.219391  0.097788   \n",
       "3 -1.463328 -0.539312  0.655594    0.769418     0.046954  0.047742   \n",
       "4  1.303308 -0.462913  0.622415    0.769418    -0.678310  1.773991   \n",
       "\n",
       "   time_signature  audio_valence  label  \n",
       "0        0.136944      -0.220610      1  \n",
       "1        0.136944      -0.645751      0  \n",
       "2        0.136944      -0.833793      1  \n",
       "3        0.136944      -1.348867      1  \n",
       "4        0.136944       0.188178      2  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train , x_test , y_train , y_test = train_test_split(df.drop('label' , axis = 1) , df['label'] , test_size = 0.1 , random_state = 37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_duration_ms</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>audio_mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>audio_valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10960</th>\n",
       "      <td>-1.089197</td>\n",
       "      <td>2.384601</td>\n",
       "      <td>-0.251075</td>\n",
       "      <td>-1.592726</td>\n",
       "      <td>0.541517</td>\n",
       "      <td>1.026644</td>\n",
       "      <td>-0.599737</td>\n",
       "      <td>-2.065341</td>\n",
       "      <td>-1.299684</td>\n",
       "      <td>-0.700346</td>\n",
       "      <td>-1.430647</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-1.626843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12925</th>\n",
       "      <td>0.375721</td>\n",
       "      <td>-0.883440</td>\n",
       "      <td>-1.303918</td>\n",
       "      <td>1.639483</td>\n",
       "      <td>-0.351559</td>\n",
       "      <td>-1.186664</td>\n",
       "      <td>1.002565</td>\n",
       "      <td>0.812344</td>\n",
       "      <td>-1.299684</td>\n",
       "      <td>0.305635</td>\n",
       "      <td>1.297773</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-1.402010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>0.886592</td>\n",
       "      <td>-0.892965</td>\n",
       "      <td>-0.997636</td>\n",
       "      <td>1.107009</td>\n",
       "      <td>0.595673</td>\n",
       "      <td>-1.463328</td>\n",
       "      <td>1.808231</td>\n",
       "      <td>0.768454</td>\n",
       "      <td>-1.299684</td>\n",
       "      <td>-0.409091</td>\n",
       "      <td>1.843784</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-0.756124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12423</th>\n",
       "      <td>0.122356</td>\n",
       "      <td>-0.071140</td>\n",
       "      <td>-0.212790</td>\n",
       "      <td>0.121466</td>\n",
       "      <td>-0.352046</td>\n",
       "      <td>-1.463328</td>\n",
       "      <td>-0.108698</td>\n",
       "      <td>0.672052</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>-0.122626</td>\n",
       "      <td>0.303299</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-1.136297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17343</th>\n",
       "      <td>0.311499</td>\n",
       "      <td>-0.202760</td>\n",
       "      <td>0.584819</td>\n",
       "      <td>1.536725</td>\n",
       "      <td>-0.351982</td>\n",
       "      <td>0.196654</td>\n",
       "      <td>-0.143424</td>\n",
       "      <td>0.555012</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>-0.474240</td>\n",
       "      <td>0.208292</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>0.813625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       song_duration_ms  acousticness  danceability    energy  \\\n",
       "10960         -1.089197      2.384601     -0.251075 -1.592726   \n",
       "12925          0.375721     -0.883440     -1.303918  1.639483   \n",
       "10101          0.886592     -0.892965     -0.997636  1.107009   \n",
       "12423          0.122356     -0.071140     -0.212790  0.121466   \n",
       "17343          0.311499     -0.202760      0.584819  1.536725   \n",
       "\n",
       "       instrumentalness       key  liveness  loudness  audio_mode  \\\n",
       "10960          0.541517  1.026644 -0.599737 -2.065341   -1.299684   \n",
       "12925         -0.351559 -1.186664  1.002565  0.812344   -1.299684   \n",
       "10101          0.595673 -1.463328  1.808231  0.768454   -1.299684   \n",
       "12423         -0.352046 -1.463328 -0.108698  0.672052    0.769418   \n",
       "17343         -0.351982  0.196654 -0.143424  0.555012    0.769418   \n",
       "\n",
       "       speechiness     tempo  time_signature  audio_valence  \n",
       "10960    -0.700346 -1.430647        0.136944      -1.626843  \n",
       "12925     0.305635  1.297773        0.136944      -1.402010  \n",
       "10101    -0.409091  1.843784        0.136944      -0.756124  \n",
       "12423    -0.122626  0.303299        0.136944      -1.136297  \n",
       "17343    -0.474240  0.208292        0.136944       0.813625  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scaler \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_duration_ms</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>audio_mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>audio_valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10960</th>\n",
       "      <td>-1.089197</td>\n",
       "      <td>2.384601</td>\n",
       "      <td>-0.251075</td>\n",
       "      <td>-1.592726</td>\n",
       "      <td>0.541517</td>\n",
       "      <td>1.026644</td>\n",
       "      <td>-0.599737</td>\n",
       "      <td>-2.065341</td>\n",
       "      <td>-1.299684</td>\n",
       "      <td>-0.700346</td>\n",
       "      <td>-1.430647</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-1.626843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12925</th>\n",
       "      <td>0.375721</td>\n",
       "      <td>-0.883440</td>\n",
       "      <td>-1.303918</td>\n",
       "      <td>1.639483</td>\n",
       "      <td>-0.351559</td>\n",
       "      <td>-1.186664</td>\n",
       "      <td>1.002565</td>\n",
       "      <td>0.812344</td>\n",
       "      <td>-1.299684</td>\n",
       "      <td>0.305635</td>\n",
       "      <td>1.297773</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-1.402010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>0.886592</td>\n",
       "      <td>-0.892965</td>\n",
       "      <td>-0.997636</td>\n",
       "      <td>1.107009</td>\n",
       "      <td>0.595673</td>\n",
       "      <td>-1.463328</td>\n",
       "      <td>1.808231</td>\n",
       "      <td>0.768454</td>\n",
       "      <td>-1.299684</td>\n",
       "      <td>-0.409091</td>\n",
       "      <td>1.843784</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-0.756124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12423</th>\n",
       "      <td>0.122356</td>\n",
       "      <td>-0.071140</td>\n",
       "      <td>-0.212790</td>\n",
       "      <td>0.121466</td>\n",
       "      <td>-0.352046</td>\n",
       "      <td>-1.463328</td>\n",
       "      <td>-0.108698</td>\n",
       "      <td>0.672052</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>-0.122626</td>\n",
       "      <td>0.303299</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-1.136297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17343</th>\n",
       "      <td>0.311499</td>\n",
       "      <td>-0.202760</td>\n",
       "      <td>0.584819</td>\n",
       "      <td>1.536725</td>\n",
       "      <td>-0.351982</td>\n",
       "      <td>0.196654</td>\n",
       "      <td>-0.143424</td>\n",
       "      <td>0.555012</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>-0.474240</td>\n",
       "      <td>0.208292</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>0.813625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       song_duration_ms  acousticness  danceability    energy  \\\n",
       "10960         -1.089197      2.384601     -0.251075 -1.592726   \n",
       "12925          0.375721     -0.883440     -1.303918  1.639483   \n",
       "10101          0.886592     -0.892965     -0.997636  1.107009   \n",
       "12423          0.122356     -0.071140     -0.212790  0.121466   \n",
       "17343          0.311499     -0.202760      0.584819  1.536725   \n",
       "\n",
       "       instrumentalness       key  liveness  loudness  audio_mode  \\\n",
       "10960          0.541517  1.026644 -0.599737 -2.065341   -1.299684   \n",
       "12925         -0.351559 -1.186664  1.002565  0.812344   -1.299684   \n",
       "10101          0.595673 -1.463328  1.808231  0.768454   -1.299684   \n",
       "12423         -0.352046 -1.463328 -0.108698  0.672052    0.769418   \n",
       "17343         -0.351982  0.196654 -0.143424  0.555012    0.769418   \n",
       "\n",
       "       speechiness     tempo  time_signature  audio_valence  \n",
       "10960    -0.700346 -1.430647        0.136944      -1.626843  \n",
       "12925     0.305635  1.297773        0.136944      -1.402010  \n",
       "10101    -0.409091  1.843784        0.136944      -0.756124  \n",
       "12423    -0.122626  0.303299        0.136944      -1.136297  \n",
       "17343    -0.474240  0.208292        0.136944       0.813625  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = df.columns [:-1]\n",
    "\n",
    "x_train = pd.DataFrame(x_train , columns= columns)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10960</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12925</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12423</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17343</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11898</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4118</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9036</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16951 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "10960      2\n",
       "12925      2\n",
       "10101      2\n",
       "12423      0\n",
       "17343      2\n",
       "...      ...\n",
       "11898      0\n",
       "4118       0\n",
       "988        0\n",
       "9036       1\n",
       "1935       1\n",
       "\n",
       "[16951 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train , columns= ['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_duration_ms</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>audio_mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>audio_valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10960</th>\n",
       "      <td>-1.089197</td>\n",
       "      <td>2.384601</td>\n",
       "      <td>-0.251075</td>\n",
       "      <td>-1.592726</td>\n",
       "      <td>0.541517</td>\n",
       "      <td>1.026644</td>\n",
       "      <td>-0.599737</td>\n",
       "      <td>-2.065341</td>\n",
       "      <td>-1.299684</td>\n",
       "      <td>-0.700346</td>\n",
       "      <td>-1.430647</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-1.626843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12925</th>\n",
       "      <td>0.375721</td>\n",
       "      <td>-0.883440</td>\n",
       "      <td>-1.303918</td>\n",
       "      <td>1.639483</td>\n",
       "      <td>-0.351559</td>\n",
       "      <td>-1.186664</td>\n",
       "      <td>1.002565</td>\n",
       "      <td>0.812344</td>\n",
       "      <td>-1.299684</td>\n",
       "      <td>0.305635</td>\n",
       "      <td>1.297773</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-1.402010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10101</th>\n",
       "      <td>0.886592</td>\n",
       "      <td>-0.892965</td>\n",
       "      <td>-0.997636</td>\n",
       "      <td>1.107009</td>\n",
       "      <td>0.595673</td>\n",
       "      <td>-1.463328</td>\n",
       "      <td>1.808231</td>\n",
       "      <td>0.768454</td>\n",
       "      <td>-1.299684</td>\n",
       "      <td>-0.409091</td>\n",
       "      <td>1.843784</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-0.756124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12423</th>\n",
       "      <td>0.122356</td>\n",
       "      <td>-0.071140</td>\n",
       "      <td>-0.212790</td>\n",
       "      <td>0.121466</td>\n",
       "      <td>-0.352046</td>\n",
       "      <td>-1.463328</td>\n",
       "      <td>-0.108698</td>\n",
       "      <td>0.672052</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>-0.122626</td>\n",
       "      <td>0.303299</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>-1.136297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17343</th>\n",
       "      <td>0.311499</td>\n",
       "      <td>-0.202760</td>\n",
       "      <td>0.584819</td>\n",
       "      <td>1.536725</td>\n",
       "      <td>-0.351982</td>\n",
       "      <td>0.196654</td>\n",
       "      <td>-0.143424</td>\n",
       "      <td>0.555012</td>\n",
       "      <td>0.769418</td>\n",
       "      <td>-0.474240</td>\n",
       "      <td>0.208292</td>\n",
       "      <td>0.136944</td>\n",
       "      <td>0.813625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       song_duration_ms  acousticness  danceability    energy  \\\n",
       "10960         -1.089197      2.384601     -0.251075 -1.592726   \n",
       "12925          0.375721     -0.883440     -1.303918  1.639483   \n",
       "10101          0.886592     -0.892965     -0.997636  1.107009   \n",
       "12423          0.122356     -0.071140     -0.212790  0.121466   \n",
       "17343          0.311499     -0.202760      0.584819  1.536725   \n",
       "\n",
       "       instrumentalness       key  liveness  loudness  audio_mode  \\\n",
       "10960          0.541517  1.026644 -0.599737 -2.065341   -1.299684   \n",
       "12925         -0.351559 -1.186664  1.002565  0.812344   -1.299684   \n",
       "10101          0.595673 -1.463328  1.808231  0.768454   -1.299684   \n",
       "12423         -0.352046 -1.463328 -0.108698  0.672052    0.769418   \n",
       "17343         -0.351982  0.196654 -0.143424  0.555012    0.769418   \n",
       "\n",
       "       speechiness     tempo  time_signature  audio_valence  \n",
       "10960    -0.700346 -1.430647        0.136944      -1.626843  \n",
       "12925     0.305635  1.297773        0.136944      -1.402010  \n",
       "10101    -0.409091  1.843784        0.136944      -0.756124  \n",
       "12423    -0.122626  0.303299        0.136944      -1.136297  \n",
       "17343    -0.474240  0.208292        0.136944       0.813625  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    8757\n",
       "0    4182\n",
       "1    2334\n",
       "3    1678\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  value counts \n",
    "\n",
    "y_train['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot silhouette and elbow graphs , then add cluster labels as preprocessing \n",
    "\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# from sklearn.metrics import silhouette_score\n",
    "\n",
    "# elbow_score = []\n",
    "# silhouette = []\n",
    "\n",
    "# k_max = 2\n",
    "\n",
    "# silhouette_max = 0 \n",
    "\n",
    "# for k in range(2, 40):\n",
    "\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "#     kmeans.fit(x_train)\n",
    "\n",
    "#     elbow_score.append(kmeans.inertia_)\n",
    "\n",
    "#     silhouette.append(silhouette_score(x_train, kmeans.labels_))\n",
    "\n",
    "#     if silhouette[-1] > silhouette_max:\n",
    "#         silhouette_max = silhouette[-1]\n",
    "#         k_max = k\n",
    "\n",
    "# print(max(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot graph \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(range(2, 40), elbow_score)\n",
    "# plt.title('Elbow Graph')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(range(2, 40), silhouette)\n",
    "# plt.title('Silhouette Graph')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Silhouette Score')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(k_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding k means as label \n",
    "\n",
    "# kmeans = KMeans(n_clusters=k_max)\n",
    "\n",
    "# kmeans.fit(x_train)\n",
    "\n",
    "# x_train['cluster_label'] = kmeans.labels_\n",
    "\n",
    "# x_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding to test \n",
    "\n",
    "# x_test = pd.DataFrame(x_test , columns= columns)\n",
    "\n",
    "# kmeans.predict(x_test) \n",
    "\n",
    "# pred =  kmeans.predict(x_test)\n",
    "\n",
    "# x_test['cluster_label'] = pred\n",
    "\n",
    "# x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada=AdaBoostClassifier(n_estimators=50,learning_rate=0.001)\n",
    "ada.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6358811040339702"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. KNN \n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors= 1)\n",
    "\n",
    "knn.fit(x_train , y_train)\n",
    "\n",
    "knn.score(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7701698513800425"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random forest \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "rf = RandomForestClassifier(n_estimators= 1000 , n_jobs=-1)\n",
    "\n",
    "rf.fit(x_train , y_train)\n",
    "\n",
    "rf.score(x_test , y_test)\n",
    "print(classification_report(rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6305732484076433"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decition tree \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt.fit(x_train , y_train)\n",
    "\n",
    "dt.score(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.35774946921443734"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naive byes \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "\n",
    "nb.fit(x_train , y_train)\n",
    "\n",
    "nb.score(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.529723991507431"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(x_train , y_train)\n",
    "\n",
    "lr.score(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5302547770700637"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM linear \n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC()\n",
    "\n",
    "svc.fit(x_train , y_train)\n",
    "\n",
    "svc.score(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5498938428874734"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM rbf \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_rbf = SVC(gamma= 0.01 , C = 500, kernel='rbf')\n",
    "\n",
    "svc_rbf.fit(x_train , y_train)\n",
    "\n",
    "svc_rbf.score(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7139065817409767"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "rf_feature_bagging = RandomForestClassifier(n_estimators=500, max_features=0.9, random_state=42)\n",
    "rf_bootstrapping = RandomForestClassifier(n_estimators=500, bootstrap=True, random_state=42)\n",
    "rf_no_bootstrapping = RandomForestClassifier(n_estimators=500, bootstrap=False, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf_feat_bag', rf_feature_bagging),\n",
    "        ('rf_boot', rf_bootstrapping),\n",
    "        ('rf_no_boot', rf_no_bootstrapping),\n",
    "        ('knn', knn)\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "ensemble.fit(x_train, y_train)\n",
    "\n",
    "ensemble.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1105: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.29846648\n",
      "Iteration 2, loss = 1.19395335\n",
      "Iteration 3, loss = 1.18614612\n",
      "Iteration 4, loss = 1.18438210\n",
      "Iteration 5, loss = 1.18323362\n",
      "Iteration 6, loss = 1.18223317\n",
      "Iteration 7, loss = 1.18128411\n",
      "Iteration 8, loss = 1.18037769\n",
      "Iteration 9, loss = 1.17954063\n",
      "Iteration 10, loss = 1.17865492\n",
      "Iteration 11, loss = 1.17778440\n",
      "Iteration 12, loss = 1.17693943\n",
      "Iteration 13, loss = 1.17611332\n",
      "Iteration 14, loss = 1.17521734\n",
      "Iteration 15, loss = 1.17436267\n",
      "Iteration 16, loss = 1.17346722\n",
      "Iteration 17, loss = 1.17263812\n",
      "Iteration 18, loss = 1.17173110\n",
      "Iteration 19, loss = 1.17074138\n",
      "Iteration 20, loss = 1.16984905\n",
      "Iteration 21, loss = 1.16886945\n",
      "Iteration 22, loss = 1.16783417\n",
      "Iteration 23, loss = 1.16694637\n",
      "Iteration 24, loss = 1.16591985\n",
      "Iteration 25, loss = 1.16492084\n",
      "Iteration 26, loss = 1.16407680\n",
      "Iteration 27, loss = 1.16321304\n",
      "Iteration 28, loss = 1.16237379\n",
      "Iteration 29, loss = 1.16157544\n",
      "Iteration 30, loss = 1.16082075\n",
      "Iteration 31, loss = 1.16001205\n",
      "Iteration 32, loss = 1.15928776\n",
      "Iteration 33, loss = 1.15862996\n",
      "Iteration 34, loss = 1.15784845\n",
      "Iteration 35, loss = 1.15717171\n",
      "Iteration 36, loss = 1.15656268\n",
      "Iteration 37, loss = 1.15591699\n",
      "Iteration 38, loss = 1.15525975\n",
      "Iteration 39, loss = 1.15461214\n",
      "Iteration 40, loss = 1.15405650\n",
      "Iteration 41, loss = 1.15351725\n",
      "Iteration 42, loss = 1.15292360\n",
      "Iteration 43, loss = 1.15251215\n",
      "Iteration 44, loss = 1.15193751\n",
      "Iteration 45, loss = 1.15153874\n",
      "Iteration 46, loss = 1.15100468\n",
      "Iteration 47, loss = 1.15052595\n",
      "Iteration 48, loss = 1.15006346\n",
      "Iteration 49, loss = 1.14969721\n",
      "Iteration 50, loss = 1.14927715\n",
      "Iteration 51, loss = 1.14882367\n",
      "Iteration 52, loss = 1.14843760\n",
      "Iteration 53, loss = 1.14807082\n",
      "Iteration 54, loss = 1.14773744\n",
      "Iteration 55, loss = 1.14730390\n",
      "Iteration 56, loss = 1.14687881\n",
      "Iteration 57, loss = 1.14648558\n",
      "Iteration 58, loss = 1.14623135\n",
      "Iteration 59, loss = 1.14591623\n",
      "Iteration 60, loss = 1.14546491\n",
      "Iteration 61, loss = 1.14516243\n",
      "Iteration 62, loss = 1.14485049\n",
      "Iteration 63, loss = 1.14452493\n",
      "Iteration 64, loss = 1.14423680\n",
      "Iteration 65, loss = 1.14391452\n",
      "Iteration 66, loss = 1.14359314\n",
      "Iteration 67, loss = 1.14337040\n",
      "Iteration 68, loss = 1.14306979\n",
      "Iteration 69, loss = 1.14292891\n",
      "Iteration 70, loss = 1.14251989\n",
      "Iteration 71, loss = 1.14214774\n",
      "Iteration 72, loss = 1.14192394\n",
      "Iteration 73, loss = 1.14177695\n",
      "Iteration 74, loss = 1.14132086\n",
      "Iteration 75, loss = 1.14112600\n",
      "Iteration 76, loss = 1.14089402\n",
      "Iteration 77, loss = 1.14059476\n",
      "Iteration 78, loss = 1.14032843\n",
      "Iteration 79, loss = 1.14021964\n",
      "Iteration 80, loss = 1.13991353\n",
      "Iteration 81, loss = 1.13975591\n",
      "Iteration 82, loss = 1.13937064\n",
      "Iteration 83, loss = 1.13900433\n",
      "Iteration 84, loss = 1.13891713\n",
      "Iteration 85, loss = 1.13862965\n",
      "Iteration 86, loss = 1.13842599\n",
      "Iteration 87, loss = 1.13835274\n",
      "Iteration 88, loss = 1.13784123\n",
      "Iteration 89, loss = 1.13781347\n",
      "Iteration 90, loss = 1.13744956\n",
      "Iteration 91, loss = 1.13718850\n",
      "Iteration 92, loss = 1.13699525\n",
      "Iteration 93, loss = 1.13677044\n",
      "Iteration 94, loss = 1.13662239\n",
      "Iteration 95, loss = 1.13657487\n",
      "Iteration 96, loss = 1.13617366\n",
      "Iteration 97, loss = 1.13600712\n",
      "Iteration 98, loss = 1.13577495\n",
      "Iteration 99, loss = 1.13554738\n",
      "Iteration 100, loss = 1.13516800\n",
      "Iteration 101, loss = 1.13509708\n",
      "Iteration 102, loss = 1.13511160\n",
      "Iteration 103, loss = 1.13478738\n",
      "Iteration 104, loss = 1.13449609\n",
      "Iteration 105, loss = 1.13442064\n",
      "Iteration 106, loss = 1.13420742\n",
      "Iteration 107, loss = 1.13415391\n",
      "Iteration 108, loss = 1.13363219\n",
      "Iteration 109, loss = 1.13349960\n",
      "Iteration 110, loss = 1.13337251\n",
      "Iteration 111, loss = 1.13320929\n",
      "Iteration 112, loss = 1.13305305\n",
      "Iteration 113, loss = 1.13293382\n",
      "Iteration 114, loss = 1.13267771\n",
      "Iteration 115, loss = 1.13240435\n",
      "Iteration 116, loss = 1.13234933\n",
      "Iteration 117, loss = 1.13198705\n",
      "Iteration 118, loss = 1.13179361\n",
      "Iteration 119, loss = 1.13176415\n",
      "Iteration 120, loss = 1.13161589\n",
      "Iteration 121, loss = 1.13141612\n",
      "Iteration 122, loss = 1.13110852\n",
      "Iteration 123, loss = 1.13079975\n",
      "Iteration 124, loss = 1.13080273\n",
      "Iteration 125, loss = 1.13039106\n",
      "Iteration 126, loss = 1.13031425\n",
      "Iteration 127, loss = 1.13006936\n",
      "Iteration 128, loss = 1.12988975\n",
      "Iteration 129, loss = 1.12975996\n",
      "Iteration 130, loss = 1.12965906\n",
      "Iteration 131, loss = 1.12942400\n",
      "Iteration 132, loss = 1.12918790\n",
      "Iteration 133, loss = 1.12917525\n",
      "Iteration 134, loss = 1.12888033\n",
      "Iteration 135, loss = 1.12869293\n",
      "Iteration 136, loss = 1.12848337\n",
      "Iteration 137, loss = 1.12835779\n",
      "Iteration 138, loss = 1.12803974\n",
      "Iteration 139, loss = 1.12805012\n",
      "Iteration 140, loss = 1.12764053\n",
      "Iteration 141, loss = 1.12765145\n",
      "Iteration 142, loss = 1.12739086\n",
      "Iteration 143, loss = 1.12733706\n",
      "Iteration 144, loss = 1.12701005\n",
      "Iteration 145, loss = 1.12678910\n",
      "Iteration 146, loss = 1.12651666\n",
      "Iteration 147, loss = 1.12668522\n",
      "Iteration 148, loss = 1.12641973\n",
      "Iteration 149, loss = 1.12616375\n",
      "Iteration 150, loss = 1.12593550\n",
      "Iteration 151, loss = 1.12599734\n",
      "Iteration 152, loss = 1.12568011\n",
      "Iteration 153, loss = 1.12582664\n",
      "Iteration 154, loss = 1.12543507\n",
      "Iteration 155, loss = 1.12513174\n",
      "Iteration 156, loss = 1.12520594\n",
      "Iteration 157, loss = 1.12496326\n",
      "Iteration 158, loss = 1.12476803\n",
      "Iteration 159, loss = 1.12464718\n",
      "Iteration 160, loss = 1.12451005\n",
      "Iteration 161, loss = 1.12450189\n",
      "Iteration 162, loss = 1.12424061\n",
      "Iteration 163, loss = 1.12425347\n",
      "Iteration 164, loss = 1.12396405\n",
      "Iteration 165, loss = 1.12367994\n",
      "Iteration 166, loss = 1.12385285\n",
      "Iteration 167, loss = 1.12350499\n",
      "Iteration 168, loss = 1.12345268\n",
      "Iteration 169, loss = 1.12305935\n",
      "Iteration 170, loss = 1.12332179\n",
      "Iteration 171, loss = 1.12300855\n",
      "Iteration 172, loss = 1.12282030\n",
      "Iteration 173, loss = 1.12307490\n",
      "Iteration 174, loss = 1.12243256\n",
      "Iteration 175, loss = 1.12247549\n",
      "Iteration 176, loss = 1.12205773\n",
      "Iteration 177, loss = 1.12218677\n",
      "Iteration 178, loss = 1.12199607\n",
      "Iteration 179, loss = 1.12170948\n",
      "Iteration 180, loss = 1.12165771\n",
      "Iteration 181, loss = 1.12162907\n",
      "Iteration 182, loss = 1.12146542\n",
      "Iteration 183, loss = 1.12142302\n",
      "Iteration 184, loss = 1.12123801\n",
      "Iteration 185, loss = 1.12116995\n",
      "Iteration 186, loss = 1.12090178\n",
      "Iteration 187, loss = 1.12090559\n",
      "Iteration 188, loss = 1.12084557\n",
      "Iteration 189, loss = 1.12067147\n",
      "Iteration 190, loss = 1.12047068\n",
      "Iteration 191, loss = 1.12029267\n",
      "Iteration 192, loss = 1.12045376\n",
      "Iteration 193, loss = 1.12001625\n",
      "Iteration 194, loss = 1.11981611\n",
      "Iteration 195, loss = 1.12031110\n",
      "Iteration 196, loss = 1.11977978\n",
      "Iteration 197, loss = 1.11959730\n",
      "Iteration 198, loss = 1.11970073\n",
      "Iteration 199, loss = 1.11948879\n",
      "Iteration 200, loss = 1.11924310\n",
      "Iteration 201, loss = 1.11929343\n",
      "Iteration 202, loss = 1.11939603\n",
      "Iteration 203, loss = 1.11912775\n",
      "Iteration 204, loss = 1.11883898\n",
      "Iteration 205, loss = 1.11891432\n",
      "Iteration 206, loss = 1.11904075\n",
      "Iteration 207, loss = 1.11852926\n",
      "Iteration 208, loss = 1.11858003\n",
      "Iteration 209, loss = 1.11846461\n",
      "Iteration 210, loss = 1.11856237\n",
      "Iteration 211, loss = 1.11823187\n",
      "Iteration 212, loss = 1.11859159\n",
      "Iteration 213, loss = 1.11794150\n",
      "Iteration 214, loss = 1.11784469\n",
      "Iteration 215, loss = 1.11787393\n",
      "Iteration 216, loss = 1.11800349\n",
      "Iteration 217, loss = 1.11772597\n",
      "Iteration 218, loss = 1.11804606\n",
      "Iteration 219, loss = 1.11753101\n",
      "Iteration 220, loss = 1.11755665\n",
      "Iteration 221, loss = 1.11741090\n",
      "Iteration 222, loss = 1.11736708\n",
      "Iteration 223, loss = 1.11711628\n",
      "Iteration 224, loss = 1.11736354\n",
      "Iteration 225, loss = 1.11721819\n",
      "Iteration 226, loss = 1.11697367\n",
      "Iteration 227, loss = 1.11653718\n",
      "Iteration 228, loss = 1.11668288\n",
      "Iteration 229, loss = 1.11690679\n",
      "Iteration 230, loss = 1.11671165\n",
      "Iteration 231, loss = 1.11655449\n",
      "Iteration 232, loss = 1.11655896\n",
      "Iteration 233, loss = 1.11640305\n",
      "Iteration 234, loss = 1.11649102\n",
      "Iteration 235, loss = 1.11631559\n",
      "Iteration 236, loss = 1.11596457\n",
      "Iteration 237, loss = 1.11600640\n",
      "Iteration 238, loss = 1.11613006\n",
      "Iteration 239, loss = 1.11597420\n",
      "Iteration 240, loss = 1.11581024\n",
      "Iteration 241, loss = 1.11579145\n",
      "Iteration 242, loss = 1.11552071\n",
      "Iteration 243, loss = 1.11553900\n",
      "Iteration 244, loss = 1.11545157\n",
      "Iteration 245, loss = 1.11543159\n",
      "Iteration 246, loss = 1.11511028\n",
      "Iteration 247, loss = 1.11552283\n",
      "Iteration 248, loss = 1.11517961\n",
      "Iteration 249, loss = 1.11509267\n",
      "Iteration 250, loss = 1.11500611\n",
      "Iteration 251, loss = 1.11471044\n",
      "Iteration 252, loss = 1.11466573\n",
      "Iteration 253, loss = 1.11459803\n",
      "Iteration 254, loss = 1.11494456\n",
      "Iteration 255, loss = 1.11449455\n",
      "Iteration 256, loss = 1.11456601\n",
      "Iteration 257, loss = 1.11431873\n",
      "Iteration 258, loss = 1.11480781\n",
      "Iteration 259, loss = 1.11430318\n",
      "Iteration 260, loss = 1.11415152\n",
      "Iteration 261, loss = 1.11408851\n",
      "Iteration 262, loss = 1.11402873\n",
      "Iteration 263, loss = 1.11393966\n",
      "Iteration 264, loss = 1.11393689\n",
      "Iteration 265, loss = 1.11386002\n",
      "Iteration 266, loss = 1.11396533\n",
      "Iteration 267, loss = 1.11348648\n",
      "Iteration 268, loss = 1.11387137\n",
      "Iteration 269, loss = 1.11357583\n",
      "Iteration 270, loss = 1.11350916\n",
      "Iteration 271, loss = 1.11327446\n",
      "Iteration 272, loss = 1.11327277\n",
      "Iteration 273, loss = 1.11328689\n",
      "Iteration 274, loss = 1.11310586\n",
      "Iteration 275, loss = 1.11328015\n",
      "Iteration 276, loss = 1.11291177\n",
      "Iteration 277, loss = 1.11271480\n",
      "Iteration 278, loss = 1.11302938\n",
      "Iteration 279, loss = 1.11291322\n",
      "Iteration 280, loss = 1.11299684\n",
      "Iteration 281, loss = 1.11268358\n",
      "Iteration 282, loss = 1.11253859\n",
      "Iteration 283, loss = 1.11237013\n",
      "Iteration 284, loss = 1.11253606\n",
      "Iteration 285, loss = 1.11230384\n",
      "Iteration 286, loss = 1.11253783\n",
      "Iteration 287, loss = 1.11234063\n",
      "Iteration 288, loss = 1.11193093\n",
      "Iteration 289, loss = 1.11207099\n",
      "Iteration 290, loss = 1.11199345\n",
      "Iteration 291, loss = 1.11218401\n",
      "Iteration 292, loss = 1.11203533\n",
      "Iteration 293, loss = 1.11235200\n",
      "Iteration 294, loss = 1.11199749\n",
      "Iteration 295, loss = 1.11155597\n",
      "Iteration 296, loss = 1.11166922\n",
      "Iteration 297, loss = 1.11160359\n",
      "Iteration 298, loss = 1.11149307\n",
      "Iteration 299, loss = 1.11157765\n",
      "Iteration 300, loss = 1.11178034\n",
      "Iteration 301, loss = 1.11131444\n",
      "Iteration 302, loss = 1.11095417\n",
      "Iteration 303, loss = 1.11110340\n",
      "Iteration 304, loss = 1.11103827\n",
      "Iteration 305, loss = 1.11106093\n",
      "Iteration 306, loss = 1.11106254\n",
      "Iteration 307, loss = 1.11102278\n",
      "Iteration 308, loss = 1.11114876\n",
      "Iteration 309, loss = 1.11072657\n",
      "Iteration 310, loss = 1.11103557\n",
      "Iteration 311, loss = 1.11087760\n",
      "Iteration 312, loss = 1.11055025\n",
      "Iteration 313, loss = 1.11044577\n",
      "Iteration 314, loss = 1.11038497\n",
      "Iteration 315, loss = 1.11038334\n",
      "Iteration 316, loss = 1.11041044\n",
      "Iteration 317, loss = 1.11016452\n",
      "Iteration 318, loss = 1.11045991\n",
      "Iteration 319, loss = 1.11021883\n",
      "Iteration 320, loss = 1.11027443\n",
      "Iteration 321, loss = 1.10982516\n",
      "Iteration 322, loss = 1.10994857\n",
      "Iteration 323, loss = 1.10980532\n",
      "Iteration 324, loss = 1.10984407\n",
      "Iteration 325, loss = 1.10997812\n",
      "Iteration 326, loss = 1.10975078\n",
      "Iteration 327, loss = 1.10969969\n",
      "Iteration 328, loss = 1.10952310\n",
      "Iteration 329, loss = 1.10968771\n",
      "Iteration 330, loss = 1.10964071\n",
      "Iteration 331, loss = 1.10962941\n",
      "Iteration 332, loss = 1.10956206\n",
      "Iteration 333, loss = 1.10940840\n",
      "Iteration 334, loss = 1.10933080\n",
      "Iteration 335, loss = 1.10906749\n",
      "Iteration 336, loss = 1.10924443\n",
      "Iteration 337, loss = 1.10887778\n",
      "Iteration 338, loss = 1.10893948\n",
      "Iteration 339, loss = 1.10907697\n",
      "Iteration 340, loss = 1.10910618\n",
      "Iteration 341, loss = 1.10846875\n",
      "Iteration 342, loss = 1.10877325\n",
      "Iteration 343, loss = 1.10893524\n",
      "Iteration 344, loss = 1.10885672\n",
      "Iteration 345, loss = 1.10847626\n",
      "Iteration 346, loss = 1.10820618\n",
      "Iteration 347, loss = 1.10814944\n",
      "Iteration 348, loss = 1.10820850\n",
      "Iteration 349, loss = 1.10821343\n",
      "Iteration 350, loss = 1.10824426\n",
      "Iteration 351, loss = 1.10839933\n",
      "Iteration 352, loss = 1.10791146\n",
      "Iteration 353, loss = 1.10837667\n",
      "Iteration 354, loss = 1.10813161\n",
      "Iteration 355, loss = 1.10810215\n",
      "Iteration 356, loss = 1.10811834\n",
      "Iteration 357, loss = 1.10807513\n",
      "Iteration 358, loss = 1.10789675\n",
      "Iteration 359, loss = 1.10753889\n",
      "Iteration 360, loss = 1.10742140\n",
      "Iteration 361, loss = 1.10753568\n",
      "Iteration 362, loss = 1.10769541\n",
      "Iteration 363, loss = 1.10762657\n",
      "Iteration 364, loss = 1.10758799\n",
      "Iteration 365, loss = 1.10768258\n",
      "Iteration 366, loss = 1.10724818\n",
      "Iteration 367, loss = 1.10726716\n",
      "Iteration 368, loss = 1.10701486\n",
      "Iteration 369, loss = 1.10723896\n",
      "Iteration 370, loss = 1.10696313\n",
      "Iteration 371, loss = 1.10712916\n",
      "Iteration 372, loss = 1.10700885\n",
      "Iteration 373, loss = 1.10702544\n",
      "Iteration 374, loss = 1.10669256\n",
      "Iteration 375, loss = 1.10649393\n",
      "Iteration 376, loss = 1.10687397\n",
      "Iteration 377, loss = 1.10656919\n",
      "Iteration 378, loss = 1.10662184\n",
      "Iteration 379, loss = 1.10629527\n",
      "Iteration 380, loss = 1.10680084\n",
      "Iteration 381, loss = 1.10657017\n",
      "Iteration 382, loss = 1.10628230\n",
      "Iteration 383, loss = 1.10631389\n",
      "Iteration 384, loss = 1.10640583\n",
      "Iteration 385, loss = 1.10585826\n",
      "Iteration 386, loss = 1.10622142\n",
      "Iteration 387, loss = 1.10594304\n",
      "Iteration 388, loss = 1.10600902\n",
      "Iteration 389, loss = 1.10588925\n",
      "Iteration 390, loss = 1.10588723\n",
      "Iteration 391, loss = 1.10570223\n",
      "Iteration 392, loss = 1.10542423\n",
      "Iteration 393, loss = 1.10580102\n",
      "Iteration 394, loss = 1.10566074\n",
      "Iteration 395, loss = 1.10568107\n",
      "Iteration 396, loss = 1.10542173\n",
      "Iteration 397, loss = 1.10533494\n",
      "Iteration 398, loss = 1.10572712\n",
      "Iteration 399, loss = 1.10517219\n",
      "Iteration 400, loss = 1.10555359\n",
      "Iteration 401, loss = 1.10508453\n",
      "Iteration 402, loss = 1.10490231\n",
      "Iteration 403, loss = 1.10523507\n",
      "Iteration 404, loss = 1.10471962\n",
      "Iteration 405, loss = 1.10473614\n",
      "Iteration 406, loss = 1.10488595\n",
      "Iteration 407, loss = 1.10509741\n",
      "Iteration 408, loss = 1.10461050\n",
      "Iteration 409, loss = 1.10463781\n",
      "Iteration 410, loss = 1.10439464\n",
      "Iteration 411, loss = 1.10454288\n",
      "Iteration 412, loss = 1.10443586\n",
      "Iteration 413, loss = 1.10428720\n",
      "Iteration 414, loss = 1.10445500\n",
      "Iteration 415, loss = 1.10390747\n",
      "Iteration 416, loss = 1.10435370\n",
      "Iteration 417, loss = 1.10412325\n",
      "Iteration 418, loss = 1.10421340\n",
      "Iteration 419, loss = 1.10393216\n",
      "Iteration 420, loss = 1.10397492\n",
      "Iteration 421, loss = 1.10427786\n",
      "Iteration 422, loss = 1.10390609\n",
      "Iteration 423, loss = 1.10366509\n",
      "Iteration 424, loss = 1.10375940\n",
      "Iteration 425, loss = 1.10390415\n",
      "Iteration 426, loss = 1.10364758\n",
      "Iteration 427, loss = 1.10357262\n",
      "Iteration 428, loss = 1.10346403\n",
      "Iteration 429, loss = 1.10341779\n",
      "Iteration 430, loss = 1.10313746\n",
      "Iteration 431, loss = 1.10350519\n",
      "Iteration 432, loss = 1.10304574\n",
      "Iteration 433, loss = 1.10329010\n",
      "Iteration 434, loss = 1.10331557\n",
      "Iteration 435, loss = 1.10294346\n",
      "Iteration 436, loss = 1.10325501\n",
      "Iteration 437, loss = 1.10243647\n",
      "Iteration 438, loss = 1.10272972\n",
      "Iteration 439, loss = 1.10269800\n",
      "Iteration 440, loss = 1.10254006\n",
      "Iteration 441, loss = 1.10281310\n",
      "Iteration 442, loss = 1.10225787\n",
      "Iteration 443, loss = 1.10264948\n",
      "Iteration 444, loss = 1.10232896\n",
      "Iteration 445, loss = 1.10249535\n",
      "Iteration 446, loss = 1.10192819\n",
      "Iteration 447, loss = 1.10232489\n",
      "Iteration 448, loss = 1.10279948\n",
      "Iteration 449, loss = 1.10203970\n",
      "Iteration 450, loss = 1.10160169\n",
      "Iteration 451, loss = 1.10190660\n",
      "Iteration 452, loss = 1.10165968\n",
      "Iteration 453, loss = 1.10161677\n",
      "Iteration 454, loss = 1.10174121\n",
      "Iteration 455, loss = 1.10154166\n",
      "Iteration 456, loss = 1.10163193\n",
      "Iteration 457, loss = 1.10161733\n",
      "Iteration 458, loss = 1.10163272\n",
      "Iteration 459, loss = 1.10117335\n",
      "Iteration 460, loss = 1.10111167\n",
      "Iteration 461, loss = 1.10166146\n",
      "Iteration 462, loss = 1.10104274\n",
      "Iteration 463, loss = 1.10135286\n",
      "Iteration 464, loss = 1.10113566\n",
      "Iteration 465, loss = 1.10076872\n",
      "Iteration 466, loss = 1.10095213\n",
      "Iteration 467, loss = 1.10088834\n",
      "Iteration 468, loss = 1.10059620\n",
      "Iteration 469, loss = 1.10037000\n",
      "Iteration 470, loss = 1.10094992\n",
      "Iteration 471, loss = 1.10061765\n",
      "Iteration 472, loss = 1.10027673\n",
      "Iteration 473, loss = 1.10046280\n",
      "Iteration 474, loss = 1.10044211\n",
      "Iteration 475, loss = 1.10024533\n",
      "Iteration 476, loss = 1.10007961\n",
      "Iteration 477, loss = 1.10029144\n",
      "Iteration 478, loss = 1.10003635\n",
      "Iteration 479, loss = 1.09989225\n",
      "Iteration 480, loss = 1.09985899\n",
      "Iteration 481, loss = 1.10018037\n",
      "Iteration 482, loss = 1.10041358\n",
      "Iteration 483, loss = 1.09979759\n",
      "Iteration 484, loss = 1.09992244\n",
      "Iteration 485, loss = 1.09953384\n",
      "Iteration 486, loss = 1.09953977\n",
      "Iteration 487, loss = 1.09989506\n",
      "Iteration 488, loss = 1.09924295\n",
      "Iteration 489, loss = 1.09953672\n",
      "Iteration 490, loss = 1.09937273\n",
      "Iteration 491, loss = 1.09920511\n",
      "Iteration 492, loss = 1.09915590\n",
      "Iteration 493, loss = 1.09891246\n",
      "Iteration 494, loss = 1.09887148\n",
      "Iteration 495, loss = 1.09917831\n",
      "Iteration 496, loss = 1.09883052\n",
      "Iteration 497, loss = 1.09875890\n",
      "Iteration 498, loss = 1.09882564\n",
      "Iteration 499, loss = 1.09885779\n",
      "Iteration 500, loss = 1.09905539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\KUNAL\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5398089171974523"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP \n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100, 100), max_iter=500, alpha=0.0001,\n",
    "                    solver='sgd', verbose=10,  random_state=21,tol=0.000000001)\n",
    "\n",
    "mlp.fit(x_train , y_train)\n",
    "\n",
    "mlp.score(x_test , y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfoldcrossvalidation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "101aea92ca7e61a708c0b0d459daa6f132dbb41f436a3a312cec15418a950825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
